{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated by      : Cephas Soga\\nProject name    : Synaptiq\\nAlgorithm name  : 3epsilon\\nAlg. Family     : Transformer\\nAlg. Type       : Decoder Only\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created by      : Cephas Soga\n",
    "Project name    : Synaptiq\n",
    "Algorithm name  : 3epsilon\n",
    "Alg. Family     : Transformer\n",
    "Alg. Type       : Decoder Only\n",
    "\"\"\"\n",
    "### ADAPTED FROM https://oreil.ly/J86pg\n",
    "### Using GoogleBlog Corpus for training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import re, string, os, random, json, pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting Accelerated linear algebra\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import tensorflow as tf\n",
    "# Set the number of inter- and intra-op parallelism threads\n",
    "tf.config.threading.set_inter_op_parallelism_threads(2)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "from tensorflow.keras import layers, models, losses, callbacks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 12000\n",
    "BUFFER_SIZE = 128\n",
    "MAX_LEN = 40\n",
    "EMBEDDING_DIM  = 128\n",
    "N_HEADS = 2\n",
    "FEED_FOWARD_DIM = 128\n",
    "KEY_DIM = 128\n",
    "SEED = 42\n",
    "LOAD_MODEL = False\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read Json file and extract text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681284 blogs found !\n"
     ]
    }
   ],
   "source": [
    "# load the json file\n",
    "with open(r\"C:\\Users\\hp\\Desktop\\Text_datasets\\blogs.json\") as json_file:\n",
    "    text_ds = json.load(json_file)\n",
    "# filter the data to retain 'text' key only in the dicts\n",
    "text_ds = list(\n",
    "    item[\"text\"] for item in text_ds \n",
    "    if item[\"text\"] is not None\n",
    ")\n",
    "# create a list of stopwords\n",
    "stopwords = [\"urlLink\", \"urllink\", \"click\"]\n",
    "# remove them from dataset\n",
    "text_ds = list(word for word in text_ds if word not in stopwords) \n",
    "count = len(text_ds)\n",
    "print(f'{count} blogs found !')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Tokenize the dataset extrated from JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the punctuation to treat them as separate words\n",
    "def pad_punctuation(strings):\n",
    "    strings = re.sub(f\"([{string.punctuation}, '\\n'])\", r\" \\1 \", strings)\n",
    "    strings = re.sub(\" +\", \" \", strings)\n",
    "    return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5271c439026d49c68a836533754958b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/681284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# apply the punctuation padding to the data\n",
    "text_ds = list(pad_punctuation(line) for line in tqdm(text_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random subset from text data after padding has been applied\n",
    "idx = random.randint(0, 1000)\n",
    "sample = text_ds[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to Tensorflow dataset\n",
    "text_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(text_ds).\n",
    "    batch(BATCH_SIZE)\n",
    "    .shuffle(1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectorizaton layer\n",
    "vectors = layers.TextVectorization(\n",
    "    standardize='lower',\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt the vectorization layer to the text dataset\n",
    "vectors.adapt(text_ds)\n",
    "learned_vocabulary = vectors.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "1: [UNK]\n",
      "2: .\n",
      "3: ,\n",
      "4: i\n",
      "5: the\n",
      "6: '\n",
      "7: to\n",
      "8: and\n",
      "9: a\n",
      "Learned vocabulary size: 12000\n"
     ]
    }
   ],
   "source": [
    "# display some 'token:word' mappings as well as the size of the vocabulary\n",
    "for idx, word in enumerate(learned_vocabulary[:10]):\n",
    "    print(f'{idx}: {word}')\n",
    "print('Learned vocabulary size:', len(learned_vocabulary) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   4 1001   15  417  149  109    2  531   16   16   16   24    1    3\n",
      "    4   84    4    6   48  778  184  338    2    4   45  758  244    5\n",
      " 2446    9  261    8  134  204    4   53   62    9 1766  834    2]\n"
     ]
    }
   ],
   "source": [
    "# show vectorisations results for the random sample\n",
    "sample = vectors(sample)\n",
    "print(sample.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. finalizing the text data adjustement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the inputs that will be fed foward through the model's layers\n",
    "def transformer_inputs(text):\n",
    "    \"\"\"\n",
    "    Shift the sequences by 1 position so that the target at pos. (i) is the word at pos. (i+1).\n",
    "    The model will use all the words located at pos. (0 - K) with k < i.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectors(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = text_ds.map(transformer_inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create a casual attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def casual_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Preventing informations from future tokens to flow into current token.\n",
    "    That means masking the upper half of the dot product in self attention.\n",
    "    Ones in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "np.transpose(casual_attention_mask(1, 10, 10, dtype=tf.int32)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create a Transformer Block layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.attn = layers.MultiHeadAttention(\n",
    "            num_heads, key_dim, output_shape=embed_dim\n",
    "            )\n",
    "        self.dropout1 = layers.Dropout(self.dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(self.dropout_rate)\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ffn1 = layers.Dense(self.ff_dim, activation='relu')\n",
    "        self.ffn2 = layers.Dense(self.embed_dim)\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_sahpe = tf.shape(inputs)\n",
    "        batch_size = input_sahpe[0]\n",
    "        seq_len = input_sahpe[1]\n",
    "        casual_mask = casual_attention_mask(\n",
    "            batch_size, seq_len, seq_len, tf.bool\n",
    "        )\n",
    "        attention_output, attention_scores = self.attn(\n",
    "            inputs,\n",
    "            inputs,\n",
    "            attention_mask=casual_mask,\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn1 = self.ffn1(out1)\n",
    "        ffn2 = self.ffn2(ffn1)\n",
    "        ffn_output = self.dropout2(ffn2)\n",
    "        return  (self.layernorm2(out1 + ffn_output), attention_scores)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"key_dim\": self.key_dim,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "                \"ff_dim\": self.ff_dim,\n",
    "                \"dropout_rate\": self.dropout_rate,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Token an position embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, max_len, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.max_len  = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.token_emb = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_emb = layers.Embedding(input_dim=max_len, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"max_len\": self.max_len,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Building the transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = [losses.SparseCategoricalCrossentropy(), None]\n",
    "inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "x = TokenAndPositionEmbedding(MAX_LEN, VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
    "x, attention_scores = TransformerBlock(\n",
    "    N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FOWARD_DIM\n",
    ")(x)\n",
    "outputs = layers.Dense(VOCAB_SIZE, activation='softmax')(x)\n",
    "t_eps = models.Model(inputs=inputs, outputs=[outputs, attention_scores])\n",
    "t_eps.compile(\"adam\", loss=loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 128)         1541120   \n",
      " ng (TokenAndPositionEmbedd                                      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_block (Transfo  ((None, None, 128),       165504    \n",
      " rmerBlock)                   (None, 2, None, None))             \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, None, 12000)       1548000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3254624 (12.42 MB)\n",
      "Trainable params: 3254624 (12.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "t_eps.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. load the model if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    # load model's weights\n",
    "    t_eps = models.load_model('./models/3epsilon', compile=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Train the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a TextGenerator checpoint\n",
    "class TextGenerator(callbacks.Callback):\n",
    "    def __init__(self, index_to_word, top_k=10):\n",
    "        self.index_to_word = index_to_word\n",
    "        self.word_to_index = {\n",
    "            word: index for index, word in enumerate(index_to_word)\n",
    "        }\n",
    "    \n",
    "    def sample_from(self, probs, temperature):\n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "        return np.random.choice(len(probs), p=probs), probs\n",
    "    \n",
    "    def generate(self, start_prompt, max_tokens, temperature):\n",
    "        start_tokens = [\n",
    "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
    "        ]\n",
    "        sample_token = None\n",
    "        info = []\n",
    "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
    "            x = np.array([start_tokens])\n",
    "            y, att = self.model.predict(x, verbose=0)\n",
    "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
    "            info.append(\n",
    "                {\n",
    "                    \"prompt\": start_prompt,\n",
    "                    \"word_probs\": probs,\n",
    "                    \"atts\": att[0, :, -1, :],\n",
    "                }\n",
    "            )\n",
    "            start_tokens.append(sample_token)\n",
    "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
    "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
    "        return info\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.generate(\"love is\", max_tokens=80, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextGenerator.__init__() missing 2 required positional arguments: 'start_tokens' and 'index_to_word'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m tensorboard_callback \u001b[39m=\u001b[39m callbacks\u001b[39m.\u001b[39mTensorBoard(log_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./logs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Tokenize starting prompt\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m text_generator \u001b[39m=\u001b[39m TextGenerator(learned_vocabulary)\n",
      "\u001b[1;31mTypeError\u001b[0m: TextGenerator.__init__() missing 2 required positional arguments: 'start_tokens' and 'index_to_word'"
     ]
    }
   ],
   "source": [
    "#Create a model save checkpoint\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=\"./checkpoint/checkpoint.ckpt\",\n",
    "    save_weights_only=True,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "# Tokenize starting prompt\n",
    "text_generator = TextGenerator(learned_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5323/5323 [==============================] - ETA: 0s - loss: 4.0736 - dense_2_loss: 4.0736\n",
      "generated text:\n",
      "love is why not all i dont have anything to . from now ( it ' s ' so [UNK] ) ' tomorrow night at [UNK] this week . \n",
      "\n",
      "5323/5323 [==============================] - 6491s 1s/step - loss: 4.0736 - dense_2_loss: 4.0736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2fc282a2490>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_eps.fit(\n",
    "    text_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[model_checkpoint_callback, tensorboard_callback, text_generator],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/3epsilon_architecture\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/3epsilon_architecture\\assets\n"
     ]
    }
   ],
   "source": [
    "# save the model architecture\n",
    "t_eps.save(\"./models/3epsilon_architecture\")\n",
    "# save the model weigths\n",
    "t_eps.save_weights(\"./models/3epsilon_weights\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Generate text using the transformeer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_probs(info, vocab, top_k=5):\n",
    "    for i in info:\n",
    "        highlighted_text = []\n",
    "        for word, att_score in zip(\n",
    "            i[\"prompt\"].split(), np.mean(i[\"atts\"], axis=0)\n",
    "        ):\n",
    "            highlighted_text.append(\n",
    "                '<span style=\"background-color:rgba(135,206,250,'\n",
    "                + str(att_score / max(np.mean(i[\"atts\"], axis=0)))\n",
    "                + ');\">'\n",
    "                + word\n",
    "                + \"</span>\"\n",
    "            )\n",
    "        highlighted_text = \" \".join(highlighted_text)\n",
    "        display(HTML(highlighted_text))\n",
    "\n",
    "        word_probs = i[\"word_probs\"]\n",
    "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
    "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
    "        for p, i in zip(p_sorted, i_sorted):\n",
    "            print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
    "        print(\"--------\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Save the model and get it ready for being used in Streamlit UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "filename = '3epsilon_saved_with_pickle.sav'\n",
    "pickle.dump(t_eps, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the textgenerator\n",
    "filename = 'text_generator_saved_with_pickle.sav'\n",
    "pickle.dump(text_generator, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save learned vocabulary\n",
    "filename = 'learned_vocabulary_saved_with_pickle.sav'\n",
    "pickle.dump(learned_vocabulary, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextGenerator.on_epoch_end() got an unexpected keyword argument 'max_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m info \u001b[39m=\u001b[39m text_generator\u001b[39m.\u001b[39;49mon_epoch_end(\n\u001b[0;32m      2\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mlove is\u001b[39;49m\u001b[39m\"\u001b[39;49m, max_tokens\u001b[39m=\u001b[39;49m\u001b[39m80\u001b[39;49m , temperature\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m\n\u001b[0;32m      3\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: TextGenerator.on_epoch_end() got an unexpected keyword argument 'max_tokens'"
     ]
    }
   ],
   "source": [
    "info = text_generator.on_epoch_end(\n",
    "    \"love is\", max_tokens=80 , temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(info, learned_vocabulary)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'info' is not defined"
     ]
    }
   ],
   "source": [
    "print(info, learned_vocabulary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
